{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Spacy and NLTK tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good muffins cost $3.88\n",
      "in New York.  Please buy me two of them.\n",
      "\n",
      "Thanks.\n",
      "At eight o'clock on Thursday morning, Arthur didn't feel very good.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text = \"Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\nThanks.\\nAt eight o'clock on Thursday morning, Arthur didn't feel very good.\"\n",
    "print(text)\n",
    "\n",
    "test_data = pd.Series([text]*10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.', 'At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', ',', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "words = [token.text for token in doc if not token.is_space]\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.', 'At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', ',', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "docs = nlp.pipe(test_data)\n",
    "\n",
    "words = pd.Series([[token.text for token in doc if not token.is_space] for doc in docs])\n",
    "\n",
    "print(words[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve spacy performance we can disable some of pipeline processing steps which we do not need for word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.', 'At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', ',', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp_slim = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])\n",
    "doc = nlp(text)\n",
    "\n",
    "words = [token.text for token in doc if not token.is_space]\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.', 'At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', ',', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "docs = nlp_slim.pipe(test_data)\n",
    "\n",
    "words = pd.Series([[token.text for token in doc if not token.is_space] for doc in docs])\n",
    "\n",
    "print(words[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer, TreebankWordTokenizer, WordPunctTokenizer, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using recommended word_tokenize. Accurate but not fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.', 'At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', ',', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
      "Wall time: 5.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "words = test_data.apply(word_tokenize)\n",
    "\n",
    "print(words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_tokenize tokenizes words as accurate as spacy but performace is worser: word_tokenize takes 4-5 times more time than spacy to do the same work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PunktSentenceTokenizer + TreebankWordTokenizer == word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.', 'At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', ',', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokenizer = PunktSentenceTokenizer()\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    sentences = sentence_tokenizer.tokenize(text)\n",
    "    return [word for sentence in sentences for word in word_tokenizer.tokenize(sentence)]\n",
    "\n",
    "print(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.', 'At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', ',', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
      "Wall time: 4.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "words = test_data.apply(tokenize)\n",
    "\n",
    "print(words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation of tokenize() shows basically the same quality and performace as word_tokenize. As expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPunctTokenizer fast but not accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.', 'At', 'eight', 'o', \"'\", 'clock', 'on', 'Thursday', 'morning', ',', 'Arthur', 'didn', \"'\", 't', 'feel', 'very', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "wp_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def wp_tokenize(text):\n",
    "    return wp_tokenizer.tokenize(text)\n",
    "    \n",
    "print(wp_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.', 'At', 'eight', 'o', \"'\", 'clock', 'on', 'Thursday', 'morning', ',', 'Arthur', 'didn', \"'\", 't', 'feel', 'very', 'good', '.']\n",
      "Wall time: 155 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "words = test_data.apply(wp_tokenize)\n",
    "\n",
    "print(words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordPunctTokenizer is faster than spacy but much less accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TreebankWordTokenizer alone without PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.', 'At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', ',', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "w_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def w_tokenize(text):\n",
    "    return w_tokenizer.tokenize(text)\n",
    "    \n",
    "print(w_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.', 'At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', ',', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
      "Wall time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "words = test_data.apply(w_tokenize)\n",
    "\n",
    "print(words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected TreebankWordTokenizer is not abbly to process sentences correctly. Much less accurate than spacy with slightly worse performace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conslusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy with all pipeline steps enabled demostrates terribled performance although after disabling unnecessary steps it cannot be beaten by NLTK withoput without loss of quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
